<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html xmlns="http://www.w3.org/1999/html">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=800">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body, td, th, tr, p, a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        .hp-photo {
            width: 240px;
            height: 240px;
            border-radius: 240px;
            -webkit-border-radius: 240px;
            -moz-border-radius: 240px;
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>

    <title>Xu Yan | Chinese University of Hongkong (Shenzhen)</title>
    <!--<link rel="stylesheet" type="text/css" href="/imgs/css" >-->
    <link rel="icon" type="image/jpg" href="./imgs/cuhk_icon.png">
</head>

<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody>
    <tr>
        <td>


            <!--SECTION 1 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td width="68%" valign="middle">
                        <p align="center">
                            <strong><name>Xu Yan</name></strong>
                        </p>
                        <p align="justify">I am a Ph.D. student (from Jan 2020) in the <a href="https://sse.cuhk.edu.cn/en/page/427">
                            Programme of Computer & Information Engineering</a> at the <a href="https://www.cuhk.edu.cn/en">Chinese University of Hongkong, Shenzhen (CUHK-SZ)</a>, supervised by Profs.<a href="https://mypage.cuhk.edu.cn/academics/lizhen/">
                            Zhen Li</a> and <a href="https://sse.cuhk.edu.cn/en/faculty/cuishuguang">Shuguang Cui</a>.
                            I am also a joint training Ph.D. student in the <a href="http://www.sribd.cn/">Shenzhen Research Institute of Big Data</a>.
                            Before that, I received my bachelor’s degree from the School of Mathematics of Sun Yat-sen University in July 2018 and master’s degree from the Programme of Data Science of CUHK-SZ in Dec 2019. </br>
                            </br>
                            I am currently interested in 3D computer vision, including 3D object detection, 3D semantic segmentation, 3D single object tracking and 3D scene understanding.

                            <!--                    </br></br>-->
                            <!--                    In this summer (July - Oct 2019), I was a research intern at the Augumented Reality team of <a href="http://www.a9.com/">Amazon</a> (Palo Alto, CA).-->
                            <!--                    In my M.Phil study, I interned at <a href="https://www.astri.org/">Hong Kong Applied Science and Technology Research Institute</a>.-->
                            <!--                    In my undergraduate study, I was an exchange student at <a href="http://www.upv.es/">Universitat Politècnica de València</a> (Valencia, Spain).-->

                            </br>
                        </p>
                        <p align="center">
                            <a href="mailto:xuyan1@link.cuhk.edu.cn">Email</a> /
                            <a href="https://github.com/yanx27"> Github </a> /
                            <a href="https://blog.csdn.net/weixin_39373480"> Blog </a> /
                            <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=TK4Ty0gAAAAJ">Google Scholar</a>


                        </p>
                    </td>
                    <td align="right"><img class="hp-photo" src="./imgs/photo.jpg" style="width: 240;height: 240;"></td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 3 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>News</heading>
                        <p><strong>[2021.04.29]</strong> One paper <a href="https://arxiv.org/pdf/2104.14769.pdf">PointLIE</a> is accepted to <a href="https://ijcai-21.org/">IJCAI 2021</a> (only <strong>13.9%</strong> acceptance rate)!
                        <p><strong>[2021.03.18]</strong> Our <a href="https://github.com/CurryYuan/InstanceRefer">InstanceRefer</a> achieved <strong>1st</strong> in the public leaderboard of <a href="http://kaldir.vc.in.tum.de/scanrefer_benchmark/">ScanRefer</a>!
                        <p><strong>[2021.03.01]</strong> Our paper <a href="https://github.com/CurryYuan/InstanceRefer">InstanceRefer</a> is on arXiv!
                        <p><strong>[2021.01.09]</strong> One paper is accepted to <a href="https://biomedicalimaging.org/2021/">ISBI 2021</a>!
                        <p><strong>[2020.12.02]</strong> One paper is accepted to <a href="https://aaai.org/Conferences/AAAI-21/">AAAI 2021</a>, in which our method <a href="https://github.com/yanx27/JS3C-Net">JS3C-Net</a> achieved <strong>3rd</strong> and <strong>1st</strong> in the public leaderboard of
                        <a href="http://semantic-kitti.org/dataset.html#download">SemanticKITTI</a> in semantic segmentation and scene completion tasks!
                        <p><strong>[2020.07.03]</strong> One paper <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700052.pdf">CIMR-SR</a> is accepted to <a href="https://eccv2020.eu/">ECCV 2020</a>!
                        <p><strong>[2020.02.24]</strong> One paper <a href="https://arxiv.org/abs/2003.00492">PointASNL</a> is accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>!
                        <p><strong>[2020.01.13]</strong> One paper is accepted to <a href="https://biomedicalimaging.org/2020/wp-content/uploads/static-html-to-wp/data/dff0d41695bbae509355435cd32ecf5d/index.htm">ISBI 2020</a>!

                    </td>
                </tr>
                </tbody>
            </table>

            <!--SECTION 4 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Publications / Preprints</heading>
                    </td>
                </tr>
                </tbody>
            </table>


            <!--SECTION 5 -->
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                                <tr>
                    <td width="20%"><img src="./imgs/InstanceRefer.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/pdf/2103.01128.pdf">
                            <papertitle>InstanceRefer: Cooperative Holistic Understanding for Visual Grounding on Point Clouds through Instance Multi-level Contextual Referring
                            </papertitle>
                        </a>
                            <br>Z. Yuan*, <strong>X. Yan*</strong>, Y. Liao, R. Zhang, Z. Li, S. Cui<br>
                            <em>ArXiv, 2021 (* indicates equal contribution)<br>
                            <a href="https://arxiv.org/pdf/2103.01128.pdf">Paper</a> /
                            <a href="https://github.com/CurryYuan/InstanceRefer"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=CurryYuan&repo=InstanceRefer&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">This paper proposes a new model, named InstanceRefer, to achieve superior 3D visual grounding through the grounding-by-matching strategy. For each instance, we perform the multi-level contextual inference, i.e., referring from instance attribute perception, instance-to-instance relation perception, and instance-to-background global localization perception, respectively. InstanceRefer outperforms previous state-of-the-arts on ScanRefer online benchmark (ranked 1st place) and Nr3D/Sr3D datasets.</p>
                        <p></p>
                    </td>
                </tr>

                <tr>
                    <td width="20%"><img src="./imgs/pointlie.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://arxiv.org/pdf/2104.14769.pdf">
                            <papertitle>PointLIE: Locally Invertible Embedding for Point Cloud Sampling and Recovery
                            </papertitle>
                        </a>
                            <br>W. Zhao*, <strong>X. Yan*</strong>, J. Gao, R. Zhang, J. Zhang, Z. Li, S. Wu, S. Cui<br>
                            <em>IJCAI, 2021 (* indicates equal contribution)<br>
                            <a href="https://arxiv.org/pdf/2104.14769.pdf">Paper</a> /
                            <a href="https://github.com/zwb0/PointLIE"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=zwb0&repo=PointLIE&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">We address a fundamental problem about Point Cloud Sampling and Recovery in this paper. How to downsample the dense point cloud with arbitrary scales
                                            while preserving the local topology of discarding points in a case-agnostic manner (i.e., without additional storage for point relationship)? We propose a
                        novel Locally Invertible Embedding for point cloud adaptive sampling and recovery (PointLIE).</p>
                        <p></p>
                    </td>
                </tr>


                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                    <tr>
                        <td width="20%"><img src="./imgs/Morphology.png" alt="PontTuset" width="180"
                                             style="border-style: none"></td>
                        <td width="80%" valign="top">
                            <p><a href="https://ieeexplore.ieee.org/abstract/document/9433850">
                                <papertitle>Geometric Morphology based Irrelevant Vessels Removal for Accurate Coronary Artery Segmentation
                                </papertitle>
                            </a>
                                <br>Q. Wang,* W. Zhao*, <strong>X. Yan*</strong>, H. Che, K. Ye, Y. Lu, Z. Li, S. Cui<br>
                                <em> ISBI, 2021 (* indicates equal contribution)</em>
                                <br>
                                <!--<font color="red"><strong>..</strong></font><br>-->
                                <a href="https://ieeexplore.ieee.org/abstract/document/9433850">Paper</a> /
                                <a href="https://app.tmp.link/index.html?tmpui_page=/file&ukey=5f5all99b2540"><font
                                    color="red">Dataset</font> (CORONARY-48)</a>

                            <p align="justify" style="font-size:13px"> Accurate semantic segmentation of coronary artery CT images is critical in both coronary-related disease diagnosis (e.g., stenosis detection and plaque grading) and further intervention treatments. This paper incorporates the voxel and point cloud-based segmentation methods into a coarse-to-fine framework for accurate coronary artery segmentation from Coronary Computed Tomography Angiography (CCTA) images. Furthermore, we released the first annotated CCTA dataset 'CORONARY-48' for coronary artery segmentation.</p>
                       <p></p>
                        </td>
                    </tr>


                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                        <tr>
                            <td width="20%"><img src="./imgs/JS3C-Net.gif" alt="PontTuset" width="180"
                                                 style="border-style: none"></td>
                            <td width="80%" valign="top">
                                <p><a href="https://arxiv.org/pdf/2012.03762.pdf">
                                    <papertitle>Sparse Single Sweep LiDAR Point Cloud Segmentation via Learning Contextual Shape Priors from Scene Completion
                                    </papertitle>
                                </a>
                                    <br><strong>X. Yan</strong>, J. Gao, J. Li, R. Zhang, Z. Li, R. Huang, S. Cui<br>
                                    <em> AAAI, 2021</em>
                                    <br>
                                    <!--<font color="red"><strong>..</strong></font><br>-->
                                    <a href="https://arxiv.org/pdf/2012.03762.pdf">Paper</a> /
                                    <a href="https://github.com/yanx27/JS3C-Net"><font color="red">Code</font></a>
                                    <iframe src="https://ghbtns.com/github-btn.html?user=yanx27&repo=JS3C-Net&type=star&count=true&size=small"
                                            frameborder="0" scrolling="0" width="120px" height="20px"></iframe>
                                <p align="justify" style="font-size:13px">We propose a novel sparse LiDAR point cloud semantic segmentation framework assisted by learned contextual shape priors. By merging multiple frames in the LiDAR sequence as supervision, we use an auxiliary task of scene completion to learn the contextual shape priors from sequential LiDAR data. Our proposed JS3C-Net achieved 3rd and 1st in the public leaderboard of SemanticKITTI in semantic segmentation and scene completion tasks at the submission time.</p>
                                <p></p>
                            </td>
                        </tr>


                        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tbody>
                            <tr>
                                <td width="20%"><img src="./imgs/CIMR.png" alt="PontTuset" width="180"
                                                     style="border-style: none"></td>
                                <td width="80%" valign="top">
                                    <p><a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700052.pdf">
                                        <papertitle>Towards Content-Independent Multi-Reference Super-Resolution:
                                            Adaptive Pattern Matching and Feature Aggregation
                                        </papertitle>
                                    </a>
                                        <br><strong>X. Yan</strong>, W. Zhao, K. Yuan, R. Zhang, Z. Li, S. Cui<br>
                                        <em> ECCV, 2020</em>
                                        <br>
                                        <!--<font color="red"><strong>..</strong></font><br>-->
                                        <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700052.pdf">Paper</a>


                                    <p align="justify" style="font-size:13px"> This work investigates a novel multi-reference-based
                                    super-resolution problem by proposing a Content Independent Multi-Reference
                                    Super-Resolution (CIMR-SR) model, which is
                                    able to adaptively match the visual pattern between references and target
                                    image in the low resolution and enhance the feature representation of the
                                    target image in the higher resolution.</p>
                                    <p></p>
                                </td>
                            </tr>


                            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                <tbody>
                                <tr>
                                    <td width="20%"><img src="./imgs/PointASNL.png" alt="PontTuset" width="180"
                                                         style="border-style: none"></td>
                                    <td width="80%" valign="top">
                                        <p><a href="https://arxiv.org/pdf/2003.00492.pdf">
                                            <papertitle>PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling</papertitle>
                                        </a>
                                            <br><strong>X. Yan</strong>, C. Zheng, Z. Li, S. Wang, S. Cui<br>
                                            <em>CVPR,
                                                2020 </em><br>
                                            <!--<font color="red"><strong>..</strong></font><br>-->
                                            <a href="https://arxiv.org/pdf/2003.00492.pdf">Paper</a> /
                                            <a href="https://www.youtube.com/watch?v=8YYAXsLlWrY">Video</a> /
                                            <a href="https://polysz.cuhk.edu.cn/zh-hans/node/14160">News</a> /
                                            <a href="https://github.com/yanx27/PointASNL"><font color="red">Code</font></a>
                                            <iframe src="https://ghbtns.com/github-btn.html?user=yanx27&repo=PointASNL&type=star&count=true&size=small"
                                                    frameborder="0" scrolling="0" width="120px" height="20px"></iframe>

                                        <p align="justify" style="font-size:13px"> Raw point clouds data inevitably contains outliers or noise through acquisition from 3D sensors or reconstruction algorithms. This paper presents a novel end-to-end network for robust point clouds processing, named PointASNL, which can deal with point clouds with noise effectively. PointASNL achieves state-of-the-art robustness performance for classification and segmentation tasks on all datasets and significantly outperforms previous methods on the SemanticKITTI dataset with considerate noise.</p>
                                        <p></p>
                                    </td>
                                </tr>

                                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                                    <tbody>
                                    <tr>
                                        <td width="20%"><img src="./imgs/ISBI20.png" alt="PontTuset"
                                                             width="180"
                                                             style="border-style: none"></td>
                                        <td width="80%" valign="top">
                                            <p><a href="https://ieeexplore.ieee.org/document/9098325">
                                                <papertitle>An Efficient Hybrid Model for Kidney Tumor Segmentation in CT Images
                                                </papertitle>
                                            </a>
                                                <br><strong>X. Yan</strong>, K. Yuan, W. Zhao, S. Wang, Z. Li, S. Cui<br>
                                                <em> ISBI, 2020<br>
                                                <!--<font color="red"><strong>..</strong></font><br>-->
                                                <a href="https://ieeexplore.ieee.org/document/9098325">Paper</a>
                                            <p align="justify" style="font-size:13px">We propose a novel effective hybrid model for kidney tumor segmentation in CT images, which contains Foreground Segmentation Network and Sparse Segmentation Network. Our model can take all foreground as input for better contextual learning in a memory-efficient manner and consider the anisotropy of CT images. Experiments show that our model can achieve state-of-the-art tumor segmentation while reducing GPU resource demand significantly.</p>
                                        </td>
                                    </tr>
                                    </table>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                <tr>
                    <td>
                        <heading>Selected Projects</heading>
                    </td>
                </tr>
                </tbody>
            </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                                <tr>
                    <td width="20%"><img src="./imgs/pointnet.png" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch">
                            <papertitle>PointNet and PointNet++ (PyTorch)
                            </papertitle>
                        </a>
                            <br>
                            <a href="https://github.com/yanx27/Pointnet_Pointnet2_pytorch"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=yanx27&repo=Pointnet_Pointnet2_pytorch&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">PointNet and PointNet++ implemented by PyTorch (in pure Python).</p>
                        <p></p>
                    </td>
                </tr>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tbody>
                                <tr>
                    <td width="20%"><img src="./imgs/EverybodyDanceNow.gif" alt="PontTuset" width="180"
                                         style="border-style: none"></td>
                    <td width="80%" valign="top">
                        <p><a href="https://github.com/yanx27/EverybodyDanceNow_reproduce_pytorch">
                            <papertitle>EverybodyDanceNow (PyTorch)
                            </papertitle>
                        </a>
                            <br>
                            <a href="https://github.com/yanx27/EverybodyDanceNow_reproduce_pytorch"><font
                                    color="red">Code</font></a>
                            <iframe src="https://ghbtns.com/github-btn.html?user=yanx27&repo=EverybodyDanceNow_reproduce_pytorch&type=star&count=true&size=small"
                                    frameborder="0" scrolling="0" width="120px"
                                    height="20px"></iframe>
                        <p align="justify" style="font-size:13px">EverybodyDanceNow reproduced in PyTorch.</p>
                        <p></p>
                    </td>
                </tr>

<br>
                                            <table width="100%" align="center" border="0" cellspacing="0"
                                                   cellpadding="20">
                                                <tr>
                                                    <td width="100%" valign="middle">
                                                        <heading>Teaching Assistant</heading>
                                                        <ul style="list-style-type:disc;">
                                                            <li>
                                                           CIE6032: Selected Topics in Deep Learning Foundations and Their Applications (2020 Spring)

                                                            <li>
                                                           CSC3002: Introduction to Computer Science: Programming Paradigms (2020 Fall)

                                                           <li>
                                                           CIE6032: Selected Topics in Deep Learning Foundations and Their Applications (2021 Spring)

                                                        </ul>
                                                    </td>
                                                </tr>
                                            </table>



                                            <!--SECTION 10 -->
                                            <table width="100%" align="center" border="0" cellspacing="0"
                                                   cellpadding="20">
                                                <tbody>
                                                <tr>
                                                    <td><br>
                                                        <!--<p align="right"><font size="3">Erd&ouml;s = ? </font><br> -->
                                                        <p align="right"><font size="2"> Last update: 2021.06.28.</font>
                                                        </p>
                                                    </td>
                                                </tr>
                                                </tbody>
                                            </table>


                                            </td>
                                            </tr>
                                            </tbody>
                                        </table>
</body>
</html>
